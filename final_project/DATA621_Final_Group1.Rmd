---
title: 'Final: Texas Higher Education Opportunity Project (THEOP)'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

# libraries
library(tidyverse)
library(kableExtra)
library(MASS) # glm.nb()
library(mice)
library(pscl) # zeroinfl()
library(skimr)
library(sjPlot)
library(mpath)
library(yardstick)
library(labelled)
library(haven)
library(corrplot)
library(Hmisc)
library(jtools)
library(caret)
library(broom)

 # library
library(treemap)
library(ggplot2)
library(hrbrthemes)
library(viridis)
library(vip)

library(ggpubr)
library(rockchalk)



# ggplot
theme_set(theme_light())


# random seed
set.seed(42)
```




```{r common functions}

#' nice_table
#' 
#' @param df
#' @param fw
nice_table <- function(df, cap=NULL, cols=NULL, dig=3, fw=F){
  if (is.null(cols)) {c <- colnames(df)} else {c <- cols}
  table <- df %>% 
    kable(caption=cap, col.names=c, digits=dig) %>% 
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      html_font = 'monospace',
      full_width = fw)
  return(table)
}

model_diag <- function(model){
  model_sum <- summary(model)
  aic <- AIC(model)
  ar2 <- model_sum$adj.r.squared
  disp <- sum(resid(model,'pearson')^2)/model$df.residual
  loglik <- logLik(model)
  
  vec <- c(ifelse(is.null(aic), NA, aic),
           ifelse(is.null(ar2), NA, ar2),
           ifelse(is.null(disp), NA, disp),
           ifelse(is.null(loglik), NA, loglik))
  
  names(vec) <- c('AIC','Adj R2','Dispersion','Log-Lik')
  return(vec)
}



factor_haven <- function(df, col_lst) {
  for (c in col_lst) {
    df[c] <- as_factor(zap_missing(df[c]))
  }
  return(df)
}


#' Title
#'
#' @param fit
#' @param lambda
#'
#' @return
#' @export
#'
#' @examples
glmnet_cv_aicc <- function(fit, lambda = 'lambda.1se'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}


#' coeff2dt
#'
#' @param fitobject 
#' @param s 
#'
#' @return
#' @export
#'
#' @examples
coeff2dt <- function(fitobject, s) {
  coeffs <- coef(fitobject, s) 
  coeffs.dt <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) 

  # reorder the variables in term of coefficients
  return(coeffs.dt[order(coeffs.dt$coefficient, decreasing = T),])
}




#' Title
#'
#' @param df 
#' @param y 
#' @param c_df 
#'
#' @return
#' @export
#'
#' @examples
glm_coef <- function(df, y, c_df) {

  am_df <- df %>% dplyr::filter(yeardes == y & termdes=="Fall") %>% dplyr::select(!c(enroll,yeardes,termdes))
  
  am_model <- glm(am_df, formula = admit ~ . , family = binomial(link = "probit"))
  summary(am_model)
  
  am_model_aic <- am_model %>% stepAIC(trace = FALSE)
  summ(am_model_aic)
  
  a <- as.data.frame(summary(am_model_aic)$coefficients[,1])
  a <- cbind(coef = rownames(a), a)
  rownames(a) <- NULL
  names(a) <- c('coef',y)
  a$model <- 'glm'
  
  if (is.null(c_df)) {
    c_df <- a
  } else {
    c_df <- c_df %>% full_join( a, by=c('coef','model'))
  }
  
  return (c_df)
}




#' Title
#'
#' @param df 
#' @param y 
#' @param c_df 
#'
#' @return
#' @export
#'
#' @examples
lasso_coef <- function(df, y, c_df) {

  if (y == 1998) {
    am_df <- df %>% dplyr::filter(yeardes == y & termdes=="Fall") %>% dplyr::select(!c(enroll,yeardes,termdes))
  } else {
    am_df <- df %>% dplyr::filter(yeardes == y) %>% dplyr::select(!c(enroll,yeardes,termdes))
  }

  X <- model.matrix(admit ~ . , data=am_df)[,-1]
  Y <- am_df[,"admit"] 
  
  
  
  lasso.model<- cv.glmnet(x=X,y=Y,
                         family = "binomial", 
                         link = "probit",
                         standardize = TRUE,                       #standardize  
                         nfold = 5,
                         alpha=1)                                  #alpha=1 is lasso
  
  l.min <- lasso.model$lambda.min
  coef(lasso.model, s = "lambda.min" )
  
  
  a <- coeff2dt(fitobject = lasso.model, s = "lambda.min")
  names(a) <- c('coef',y)
  a$model <- 'lasso'
  
  if (is.null(c_df)) {
    c_df <- data.frame(a)
  } else {
    c_df <- c_df %>% full_join( a, by=c('coef','model'))
  }
  
  
  return (c_df)
}




#' Title
#'
#' @param df 
#' @param model 
#' @param y 
#' @param d_df 
#'
#' @return
#' @export
#'
#' @examples
demographicCount <- function(df, model, y, d_df) {
  
  if (y == 1998) {
    am_df <- df %>% dplyr::filter(yeardes == y & termdes=="Fall") %>% dplyr::select(!c(enroll,yeardes,termdes))
  } else {
    am_df <- df %>% dplyr::filter(yeardes == y) %>% dplyr::select(!c(enroll,yeardes,termdes))
  }
  
  am_pred <- predict.glm(model, am_df, "response")
  am_df$prob_admit <- am_pred
  am_df$pred_admit <- ifelse(am_pred >= 0.5, 1, 0)
  
  log_matrix_1 <- confusionMatrix(factor(am_df$pred_admit), 
                                factor(am_df$admit), "1")
  
  log_matrix_1

  
  d <- am_df %>% 
    group_by(admit,ethnic) %>%
    summarise(y = n())
  
  names(d) <- c('admit', 'ethnic', y)
  d_df <- d_df %>% full_join( d, by=c('admit','ethnic'))
  
  return(d_df)
}




#' Title
#'
#' @param df 
#' @param l_model 
#' @param y 
#' @param d_df 
#'
#' @return
#' @export
#'
#' @examples
demographicCountLasso <- function(df, l_model, y, d_df) {
  
  
  if (y == 1998) {
    am_df <- df %>% dplyr::filter(yeardes == y & termdes=="Fall") %>% dplyr::select(!c(enroll,yeardes,termdes))
  } else {
    am_df <- df %>% dplyr::filter(yeardes == y) %>% dplyr::select(!c(enroll,yeardes,termdes))
  }
  
  
  X_test <- model.matrix(admit ~ . ,data=am_df)[,-1]
  Y_test <- am_df[,"admit"] 
  
  
  # predict using coefficients at lambda.min
  lassoPred <- predict(l_model, newx = X_test, type = "response", s = 'lambda.min')
  
  #pred_df <- am_lasso1998_df 
  am_df$admit_prob <- lassoPred[,1]
  am_df$admit_pred <- ifelse(lassoPred >= 0.5, 1, 0)[,1]


  log_matrix_1 <- confusionMatrix(factor(am_df$admit_pred), 
                                factor(am_df$admit), "1")
  log_matrix_1

  
  d <- am_df %>% 
    group_by(admit,ethnic) %>%
    summarise(y = n())
  
  names(d) <- c('admit', 'ethnic', y)
  d_df <- d_df %>% full_join( d, by=c('admit','ethnic'))
  
  return(d_df)
  
}



```

# Abstract 

<i>This work examines changes in the racial and ethnic composition of admissions at Texas A&M college following the 1996 Hopwood case, which put a court prohibition on affirmative action. We estimate the extent to which these universities used affirmative action before the ban, and we examine how admissions officers at these universities adjusted the relative weights given to key applicant criteria throughout the suspension. We model the extent to which these new regulations succeeded in preserving minority admission rates at pre-Hopwood levels after examining whether changes in relative weights favored minority applicants. We discovered that most colleges followed the Hopwood rule, so that direct advantages offered to black and Hispanic candidates vanished (and, in some circumstances, became disadvantages). While there is evidence that universities changed the weights they placed on applicant characteristics other than race and ethnicity in ways that aided underrepresented minority applicants, these changes in the admissions process were unable to maintain the share of admitted students held by black and Hispanic applicants. As a result, these alternative admissions procedures have not been a reliable proxy for race and ethnicity.

</i>


# Introduction

Despite some claims, people have acknowledged a gap in access to higher education still exists, especially for minority students. However, many have objected to affirmative action policies as a solution, and in 1996 in Texas, the Fifth Court Circuit of Appeals outlawed the use of race for college admissions in the state. After a noticeable drop in minority student enrollments, the governor of Texas signed a new bill in 1997 as a compromise:  any high school student finishing in top decile received a guaranteed admission to Texas public universities (including Texas A&M University). This was known as the Texas "Top 10% Plan." Instead of competing with applicants from the entire state, students only had to compete with their immediate classmates to access higher education.

The ideas behind the the law were many. One was to continue shifting away from test based admissions (referred to as a 'shifting meritocracy') to focusing on high school grades, which some argue is a better predictor of future performance. Second, officials hoped automatically admitting students from all districts would preserve or improve diversity. Thirdly, the Top 10% law allowed colleges to be race-blind (rejecting affirmative action policies).

# Literature review
<i>Discuss how other researchers have addressed similar problems, what their achievements are, and what the advantage and drawbacks of each reviewed approach are. Explain how your investigation is similar or different to the state-of-the-art. Please cite the relevant papers where appropriate. </i>

# Methodology 
To measure the effects of both the Hopwood and Top 10% laws, we utilized a publicly available dataset called the Texas Higher Education Opportunity Project (THEOP). This dataset collects administrative data on applications, admissions and enrollment from 9 colleges and universities in the state that differ in the selectivity of their admissions, and conducts a two-cohort longitudinal survey of sophomores and seniors who were enrolled in Texas public schools as of spring, 2002.

Even though the laws affected all public schools, we narrowed our focus on Texas A&M, one of the flagship state schools where competition for admission is quite high. Again, legislators intended to both increase student diversity without explicity selecting students primarily on their ethnicity. In essence, colleges needed to find 'proxy' indicators of diversity.

We transformed most of the dataset into factors and left a few numerical predictors. Many predictors naturally fell into categories especially ethnicity and class rankings. We then created generalized linear and lasso models to see how well we could predict admission rates based on student performance and demographic predictors. If the laws had discerable and direct effects, we expected differences between the predicted and actual admission rates to Texas A&M's incoming freshman classes.


# Data

```{r}

# change to your local data dir outside the repo
#local_data_dir <- 'C:/Users/daria/Documents/theop'
local_data_dir <- '../../data/theop'


# # load application and transactions data frames
load(paste0(local_data_dir, '/data_model/df_applications.RData'))
load(paste0(local_data_dir, '/data_model/df_transcripts.RData'))

```






```{r}
# clean application data and remove labels
app_df  <- df_applications

col_lst <- c("termdes","male","ethnic","citizenship","restype","satR","actR","testscoreR","decileR","quartile","major_field","hsprivate","hstypeR","hsinstate","hseconstatus","hslos","hscentury","admit","admit_prov","enroll","gradyear","studentid_uniq","univ","termapp","sat_not_recenteredR","admit_ut_summer")

app_df <- factor_haven(app_df,col_lst)

# clean transcripts and remove labels
transcript_df <- df_transcripts

col_lst <- c("term","hrearn","term_major_dept","term_major_field")

transcript_df <- factor_haven(transcript_df,col_lst)

```


```{r}

## Choose Texas A&M college and compare the admissions policy at a university before and after top 10%. Make variables categorical if needed.
texas_applications <- filter(app_df, univ == "am") %>% dplyr::select(!c(termapp, sat_not_recenteredR, admit_ut_summer, univ)) %>% 
        drop_na(satR,decileR)
#texas_transcripts <- filter(transcript_df, univ == "am") %>% dplyr::select(!univ)
```


```{r}

## Dummy Variables for factors with two levels
dummy_vars <- function(df){
  df %>%
    mutate(
      male = factor(ifelse(male == "Male", 1, 0)), 
      US_Citizen = factor(ifelse(citizenship == "US Citizen",  1, 0)),
      Texas_resident = factor(ifelse(restype == "Texas Resident",  1, 0)),
      admit = factor(ifelse(admit == "Yes",  1, 0)),
      admit_prov = factor(ifelse(admit_prov == "Yes",  1, 0)),
      enroll = factor(ifelse(enroll == "Yes",  1, 0))
    ) %>% 
    dplyr::select(-c(citizenship,restype))
      }

texas_applications <- dummy_vars(texas_applications)


texas_applications$satR <- as.numeric(texas_applications$satR)
texas_applications$actR <- as.numeric(texas_applications$actR)
texas_applications$testscoreR <- as.numeric(texas_applications$testscoreR)
texas_applications$gradyear <- as.numeric(texas_applications$gradyear)

```



We employ administrative records from a Texas A&M University (1992-2022) that include info about admissions selectivity, public/private status, and the ethno-racial composition of their student body for this analysis. Importantly, the time period for the public organization comprises years prior to and following the judicial ban on affirmative action. This is significant because, while the judicial restriction applied to all schools in the 5th Circuit District, the top 10% policy was restricted to public colleges and universities. 
These records contain a plethora of information about the applicant pool, and have been standardized where necessary, and checked for consistency.

The **application** dataset contained 163,027 observations of 24 predictor variables and **transcripts** dataset contained 637,028 observations of 10 predictor variables, where each record represented an individual applicant.
These variables describe items typically found on a college admission application such as the year and term an applicant desired to enroll, applicant 
demographics, applicant academic characteristics, and high school characteristics.

Unfortunately, the data does not often include information regarding a student's high school academics or application essays. In evaluating the results, we take great note of these data constraints.



<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/final_project/project_data.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

Each record in the application dataset included a response variable "admit" (Institutionâ€™s admission decision), was a boolean where "1" indicated the person was admitted.

A sample of the data appears in the following table:

```{r}
DT::datatable(
      texas_applications[1:25,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```


While exploring this data, we made the following observations for the **applications data**:

-   21 variables were categorical, and 3 were numeric.
-   `actR`, `gradyear`, `hscentury`, `hslos` variables had more that 50% of the missing data.


```{r}

texas_applications %>% 
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  dplyr::rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()
```


```{r}
#remove data with NA more than 50%
texas_applications <- texas_applications %>% dplyr::select(-c(actR,gradyear,hscentury,hseconstatus,hslos))

```



76% of all the students were admitted to the university.
```{r}
texas_applications%>%
  ggplot() +
  geom_bar(aes(x=admit), colour='black', size=0.2) 
```

By checking the frequency of the variables with few unique values ("male","ethnic", "citizenship", "restype","decileR","major_field"), we checked the frequency of each value.
As demonstrated by the graph below, more than 50% of the data for the `citizenship`, `restype` and `ethnic` variables was `US Citizen`, `Texas Resident` and `White, Non-Hispanic` accordingly:

```{r discrete_plot}
texas_applications[,c("admit","male", "US_Citizen", "Texas_resident","admit_prov","enroll")] %>%
  gather("variable","value") %>%
  group_by(variable) %>%
  count(value) %>%
  mutate(value = factor(value,levels=2:0)) %>%
  mutate(percent = n*100/134020) %>%
  ggplot(.,
  aes(variable,percent)) +
  geom_bar(stat = "identity", aes(fill = value)) +
  xlab("Variable") +
  ylab("Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = rev(c("#003f5c","#58508d", "#bc5090")))
```




```{r eval=FALSE}
# a <- ggplot(texas_transcripts, aes(x = cgpa)) +
#   geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
#   labs(x = "Per semester cumulative gpa", y = "Count") 
# 
# 
# b <- ggplot(texas_transcripts, aes(x = semgpa)) +
#   geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
#   labs(x = "Semester gpa", y = "Count")
# 
# plot<- ggarrange(a, b, ncol=2, nrow=1)
# 
# annotate_figure(plot, top = text_grob("Distribution of GPA", 
#                color = "black", face = "bold", size = 14))
```



```{r eval=FALSE}
# m_df <- texas_transcripts %>% dplyr::select(c(semgpa,cgpa)) %>% melt() 
# 
# m_df %>% ggplot(aes(x= value)) + 
# geom_histogram(color='#023020', fill='gray') + 
#   facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```



```{r}
a <- ggplot(texas_applications, aes(x = quartile)) +
  geom_bar(binwidth = 1, fill = "skyblue", color = "black") +
  labs(x = "Quartile", y = "Count")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b <- ggplot(texas_applications, aes(x = decileR)) +
  geom_bar(binwidth = 1, fill = "skyblue", color = "black") +
  labs(x = "Decile", y = "Count") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


plot<- ggarrange(a, b, ncol=2, nrow=1)

annotate_figure(plot, top = text_grob("Distribution of Student HS class rank", 
               color = "black", face = "bold", size = 14))
```



# Transform Data
Students with missing values for `ethnic` variable are combined with  "White, Non-Hispanic" students, resulting in cautious estimates of policy effects.

```{r message=FALSE, warning=FALSE,}


levels(texas_applications$hsprivate) <- c(levels(texas_applications$hsprivate),"None")
texas_applications$hsprivate[is.na(texas_applications$hsprivate)] <- "None"

levels(texas_applications$hsinstate) <- c(levels(texas_applications$hsinstate),"None")
texas_applications$hsinstate[is.na(texas_applications$hsinstate)] <- "None"

#texas_applications <- texas_applications %>% mutate(ethnic = ifelse(is.na(ethnic), "5", ethnic))
#texas_applications$ethnic[is.na(texas_applications$ethnic)] <- "White, Non-Hispanic"
#texas_applications$ethnic <- as.factor(texas_applications$ethnic)

levels(texas_applications$ethnic) <- c(levels(texas_applications$ethnic),"None")
texas_applications$ethnic[is.na(texas_applications$ethnic)] <- "None"
texas_applications$ethnic <- combineLevels(texas_applications$ethnic, 
                                           levs = c("White, Non-Hispanic","None"), 
                                           newLabel = "White, Non-Hispanic")

# set values, dummy variable if top 10% or not
texas_applications <- texas_applications %>% mutate(top10 = ifelse(decileR == "Top 10%", TRUE, FALSE))
#texas_applications$top10 <- as.factor(texas_applications$top10)

levels(texas_applications$decileR) <- c(levels(texas_applications$decileR),"None")
texas_applications$decileR[is.na(texas_applications$decileR)] <- "None"


levels(texas_applications$quartile) <- c(levels(texas_applications$quartile),"None")
texas_applications$decileR[is.na(texas_applications$decileR)] <- "None"


#remove 52 observations where male=NA
texas_applications <- texas_applications %>%                                        
  filter(!is.na(male))

```



```{r message=FALSE, warning=FALSE,}
# filter 
attr_str <- c('admit', 'termdes', 'male','ethnic','US_Citizen','Texas_resident','satR','testscoreR',
              'top10','hsprivate','hsinstate','yeardes', 'enroll')

am_df <- texas_applications %>% dplyr::select(attr_str)


```



```{r message=FALSE, warning=FALSE,}

am_df %>% 
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  dplyr::rename(variable=skim_variable, min=numeric.p0, 
                max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()

```


```{r message=FALSE, warning=FALSE,}
#m_df <- merge_df %>% slice_sample(n=2000) %>%
m_df <- am_df %>%
  dplyr::select(where(is.numeric) & !c('admit','enroll')) %>% 
  pivot_longer(!c('satR'), names_to='variable' , values_to = 'value') %>% 
  drop_na()

m_df %>% ggplot(aes(x=value)) + 
#m_df %>% ggplot(aes(x=value, group=avg_gpa, fill=avg_gpa)) + 
geom_density(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```


```{r message=FALSE, warning=FALSE,}
m_df <- am_df %>%
  dplyr::select((where(is.numeric) | c('admit','enroll')) & !c(yeardes)) %>% 
  pivot_longer(!c('admit','enroll'), names_to='variable' , values_to = 'value') %>% 
  drop_na()

m_df %>% ggplot(aes(y=value, x=admit, fill=enroll)) + 
#m_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
geom_boxplot(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


# m_df %>% ggplot(aes(y=value, x=enroll)) + 
# #m_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
# geom_boxplot(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```


```{r message=FALSE, warning=FALSE,}

#m_df <- am_df 

m_df <- am_df %>%
  group_by(admit, ethnic, male) %>%
  summarise(n = n())

#treemap(m_df, index=c("ethnic","admit"), vSize="n", type="index")
treemap(m_df, index=c("admit","ethnic"), vSize="n", type="index",
      title="My Treemap",                      # Customize your title
      fontsize.title=12,
      align.labels=list(
        c("center", "center"), 
        c("right", "bottom")
        ),                                   # Where to place labels in the rectangle?
    overlap.labels=0.5,                      # 
    inflate.labels=F, )


```



```{r message=FALSE, warning=FALSE,}

m_df <- am_df %>%
  group_by(admit, ethnic, male) %>%
  summarise(n = n())

# plot
ggplot(m_df, aes(fill=ethnic, y=n, x=admit)) + 
  geom_bar(position="stack", stat="identity") + 
  scale_fill_viridis(discrete=TRUE, name="") +
  facet_wrap(~admit, scales = 'free',  ncol = 4)
  theme_ipsum() +
  ylab("Money input") + 
  xlab("Month")



```

# Models
As the first step, we built the generalized linear model based on the dataset with some variables transformed to categorical from numeric for the time before Fall, 1998.


```{r}
#no termdes=Fall or summer II only
pre_df <- am_df %>% dplyr::filter(yeardes < 1998)
b <- am_df %>% dplyr::filter(yeardes == 1998 & termdes=="Summer II")

pre_df <- rbind(pre_df,b)


demographic_df <- pre_df %>% 
  dplyr::filter(yeardes == 1997) %>%
  group_by(admit,ethnic) %>%
  summarise(pre = n())

names(demographic_df) <- c('admit','ethnic','1997')

```









```{r}
pre_df %>%
  #dplyr::select(!c(studentid)) %>%
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()

```

Since our dependent variable is binary (0 and 1), we used logistic regression. To do so, the function `glm()` with `family=binomial` was used. At the beginning, variable below were included.
```{r}

am_model_1 <- glm(pre_df, formula = admit ~ . -enroll -yeardes -termdes, family = binomial(link = "probit"))
summary(am_model_1)

```


Using StepAic() function helped to improve the model performance.

```{r}

am_model_1_aic <- am_model_1 %>% stepAIC(trace = FALSE)
summ(am_model_1_aic)

```



```{r}

a <- as.data.frame(summary(am_model_1_aic)$coefficients[,1])
a <- cbind(coef = rownames(a), a)
rownames(a) <- NULL
names(a) <- c('coef','< 1998')
a$model <- 'glm'
a <- a[, c("coef", "model", "< 1998")]

coef_tbl <- a

```


```{r}

coef_tbl <- glm_coef(am_df, '1998', coef_tbl) 
coef_tbl <- glm_coef(am_df, '1999', coef_tbl) 
coef_tbl <- glm_coef(am_df, '2000', coef_tbl) 
coef_tbl <- glm_coef(am_df, '2001', coef_tbl) 
coef_tbl <- glm_coef(am_df, '2002', coef_tbl) 

```




## Model Results


As it appeared, `ethnic` variable had the most negative effect for the university acceptance. While being Texas resident or being in top 10% could help to be admitted.

The `null deviance` of `r round(am_model_1_aic$null.deviance,2)` defined how well the target variable could be predicted by a model with only an intercept term.

The `residual deviance` of `r round(am_model_1_aic$deviance,2)` defined how well the target variable could be predicted by the AIC model that we fit with the predictor variables listed above.
The lower the value, the better the model's predictions of the response variable.

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r round(am_model_1_aic$aic,2)`.
The lower the AIC value, the better the model's ability to fit the data.


#### Checking Model Assumptions

The resulting plots show us similar to model 1 picture: under-fitting at lower predicted values, with the predicted proportions being larger than the observed proportions; over-fitting for a couple of predictor patterns at higher predicted values with the predicted values are much larger than the predicted proportions.
The Standardized Pearson Residuals plot shows an approximate Standard Normal distribution if the model fits.  The model seems good in the middle range but there are extremes on the right and left sides. Some normality of the residuals for the binomial logistic regression models  is just an evidence of a decent fitting model.

```{r check_lm2}
par(mfrow=c(2,2))
plot(am_model_1_aic)
```

By checking the linearity assumptions, we see that only `rm` seem like linear relation with the logit results.
```{r model_2_linearity}
probabilities <- predict(am_model_1_aic, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
head(predicted.classes)

#Only numeric predictors
data <- pre_df %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

```

The Standardized Residuals plot seems to have a constant variance though there are some outliers.
```{r model_2_residuals}
am_model_1_aic.data <- augment(am_model_1_aic) %>% 
  mutate(index = 1:n())

ggplot(am_model_1_aic.data, aes(index, .std.resid)) + 
  geom_point(aes(color = admit), alpha = .5) +
  theme_bw()
```

The marginal model plots below show reasonable agreement across the two sets of fits indicating that model_2_aic is a valid model.

```{r warning=FALSE, message=FALSE}
car::mmps(am_model_1_aic, span = 3/4, layout = c(2, 2))
```

In terms of multicollinearity, all variables have a VIF less than 5. As a result, multicollinearity shouldn't be a problem for our model.
```{r model_2_vif}
car::vif(am_model_1_aic)
```



# Predict

Once the uniform admission law was fully in force, fall, 1998.
```{r}

demographic_df <- demographicCount(am_df, am_model_1_aic, 1998, demographic_df)
demographic_df <- demographicCount(am_df, am_model_1_aic, 1999, demographic_df)
demographic_df <- demographicCount(am_df, am_model_1_aic, 2000, demographic_df)
demographic_df <- demographicCount(am_df, am_model_1_aic, 2001, demographic_df)
demographic_df <- demographicCount(am_df, am_model_1_aic, 2002, demographic_df)

```





# Lasso

```{r}
# set seed for consistancy 
set.seed(42)

# build X matrix and Y vector
X <- model.matrix(admit ~ . -enroll -yeardes -termdes , data=pre_df)[,-1]
Y <- pre_df[,"admit"] 


demographic_lasso_df <- pre_df %>% 
  dplyr::filter(yeardes == 1997) %>%
  group_by(admit,ethnic) %>%
  summarise(pre = n())

names(demographic_lasso_df) <- c('admit','ethnic','1997')

```


```{r}

lasso.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial", 
                       link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.model$lambda.min
l.1se <- lasso.model$lambda.1se
coef(lasso.model, s = "lambda.min" )
coef(lasso.model, s = "lambda.1se" )
lasso.model

```



```{r}

a <- coeff2dt(fitobject = lasso.model, s = "lambda.min")
names(a) <- c('coef','< 1998')
a$model <- 'lasso'
a <- a[, c("coef", "model", "< 1998")]

coef_lasso_tbl <- a

```


```{r}

coef_lasso_tbl <- lasso_coef(am_df, 1998 ,  coef_lasso_tbl)
coef_lasso_tbl <- lasso_coef(am_df, 1999 ,  coef_lasso_tbl)
coef_lasso_tbl <- lasso_coef(am_df, 2000 ,  coef_lasso_tbl)
coef_lasso_tbl <- lasso_coef(am_df, 2001 ,  coef_lasso_tbl)
coef_lasso_tbl <- lasso_coef(am_df, 2002 ,  coef_lasso_tbl)

```



```{r}

par(mfrow=c(2,2))

plot(lasso.model)
plot(lasso.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
plot(rocs,type="l")  

```



```{r}
assess.glmnet(lasso.model,           
              newx = X,              
              newy = Y )    

print(glmnet_cv_aicc(lasso.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.model, 'lambda.1se'))

```



```{r}
as.data.frame(as.matrix(coef(lasso.model, s = "lambda.min"))) %>%
  arrange(desc(s1)) %>%
  nice_table(cap='Model Coefficients', cols=c('Est'))
```



```{r}

vip(lasso.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
coeffs.table <- coeff2dt(fitobject = lasso.model, s = "lambda.min")

coeffs.table %>% mutate(name = fct_reorder(name, desc(coefficient))) %>%
ggplot() +
  geom_col(aes(y = name, x = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients with ", lambda, " = 0.0275"))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5),legend.position = "none")

```









#### Model Results


The coefficients extracted at the lambda.min value are used to predict the relative crime rate for the testing data set. The confusion matrix highlights an accuracy of 91%.



```{r}




```




```{r}
# Create matrix new data
am_lasso1998_df <- am_df %>% dplyr::filter(yeardes == 1998 & termdes=="Fall") %>%
  dplyr::select(!c(enroll,yeardes,termdes))

X_test <- model.matrix(admit ~ . ,data=am_lasso1998_df)[,-1]
Y_test <- am_lasso1998_df[,"admit"] 


# predict using coefficients at lambda.min
lassoPred <- predict(lasso.model, newx = X_test, type = "response", s = 'lambda.min')

#pred_df <- am_lasso1998_df 
am_lasso1998_df$admit_prob <- lassoPred[,1]
am_lasso1998_df$admit_pred <- ifelse(lassoPred >= 0.5, 1, 0)[,1]

```



```{r eval=FALSE}

confusion.glmnet(lasso.model, newx = X_test, newy = Y_test, s = 'lambda.min')

```






```{r}

```


```{r}

demographic_lasso_df <- demographicCountLasso(am_df, lasso.model, 1998, demographic_lasso_df)
demographic_lasso_df <- demographicCountLasso(am_df, lasso.model, 1999, demographic_lasso_df)
demographic_lasso_df <- demographicCountLasso(am_df, lasso.model, 2000, demographic_lasso_df)
demographic_lasso_df <- demographicCountLasso(am_df, lasso.model, 2001, demographic_lasso_df)
demographic_lasso_df <- demographicCountLasso(am_df, lasso.model, 2002, demographic_lasso_df)


```




# Analysis


```{r}

m_df <- coef_tbl %>%
  #dplyr::select(where(is.numeric) & !c('admit','enroll')) %>% 
  dplyr::filter(coef != '(Intercept)') %>%
  pivot_longer(!c('coef','model'), names_to='variable' , values_to = 'value') %>% 
  drop_na()


ggplot(data=m_df, aes(x=variable, y = value, colour = coef, group = coef)) + 
  geom_line() + 
  geom_point(size=0.5) +
  facet_wrap( ~coef, ncol = 4, as.table=TRUE, labeller = "label_both") +
  theme_bw()

coef_tbl %>% dplyr::select(!c(model)) %>% nice_table(cap='glm() Addmission Coefficients')

```


```{r}

m_df <- coef_lasso_tbl %>%
  #dplyr::select(where(is.numeric) & !c('admit','enroll')) %>% 
  dplyr::filter(coef != '(Intercept)') %>%
  pivot_longer(!c('coef','model'), names_to='variable' , values_to = 'value') %>% 
  drop_na()


ggplot(data=m_df, aes(x=variable, y = value, colour = coef, group = coef)) + 
  geom_line() + 
  geom_point(size=0.5) +
  facet_wrap( ~coef, ncol = 4, as.table=TRUE, labeller = "label_both") +
  theme_bw()

coef_lasso_tbl %>% dplyr::select(!c(model)) %>% nice_table(cap='Lasso Addmission Coefficients')

```





```{r}

d <- demographic_df %>% 
        pivot_longer(!c('admit','ethnic'), names_to='variable' , values_to = 'value')

d <- transform(d, percent = ave(value,variable,FUN = prop.table))

d %>% dplyr::filter(admit == 1) %>%
    ggplot(aes(x=variable, y = percent, colour = ethnic, group = ethnic)) + 
    geom_line() + 
    geom_point(size=0.5) +
    facet_wrap( ~ethnic, ncol = 4, as.table=TRUE, labeller = "label_both", scales="free_y") +
    theme_bw()



# display table
d <- d %>% dplyr::select(!c(value)) %>%
  pivot_wider(names_from = variable, values_from = percent ,names_sort = TRUE)

d %>% nice_table(cap='glm() Addmission Demographics')




```



```{r}

d <- demographic_lasso_df %>% 
        pivot_longer(!c('admit','ethnic'), names_to='variable' , values_to = 'value')

d <- transform(d, percent = ave(value,variable,FUN = prop.table))

d %>% dplyr::filter(admit == 1) %>%
    ggplot(aes(x=variable, y = percent, colour = ethnic, group = ethnic)) + 
    geom_line() + 
    geom_point(size=0.5) +
    facet_wrap( ~ethnic, ncol = 4, as.table=TRUE, labeller = "label_both", scales="free_y") +
    theme_bw()



# display table
d <- d %>% dplyr::select(!c(value)) %>%
  pivot_wider(names_from = variable, values_from = percent ,names_sort = TRUE)

d %>% nice_table(cap='glm() Addmission Demographics')

```



# Discussion and Conclusions 

<i>Conclude your findings, limitations, and suggest areas for future work. </i>

```{r}
knitr::knit_exit()
```


