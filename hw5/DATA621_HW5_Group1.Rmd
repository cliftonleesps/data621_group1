---
title: 'Homework #5: Count Regression (Wines)'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r, include=FALSE}
# if error for kableExtra, do
# devtools::install_github("kupietz/kableExtra")
```

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

# libraries
library(tidyverse)
library(kableExtra)
library(MASS) # glm.nb()
library(mice)
library(pscl) # zeroinfl()
library(skimr)
library(sjPlot)
library(mpath)
library(yardstick)
library(caret)
library(corrplot)
library(Hmisc)
library(jtools)
library(car)
library(knitr)

# ggplot
theme_set(theme_light())

```

```{css}
# only required for final knit
#h4, h5 {margin-top: 20px;}
```

```{r common functions}
nice_table <- function(df, cap=NULL, cols=NULL, dig=3, fw=F){
  if (is.null(cols)) {c <- colnames(df)} else {c <- cols}
  table <- df %>% 
    kable(caption=cap, col.names=c, digits=dig) %>% 
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      html_font = 'monospace',
      full_width = fw)
  return(table)
}

model_diag <- function(model){
  model_sum <- summary(model)
  aic <- AIC(model)
  ar2 <- model_sum$adj.r.squared
  disp <- sum(resid(model,'pearson')^2)/model$df.residual
  loglik <- logLik(model)
  
  vec <- c(ifelse(is.null(aic), NA, aic),
           ifelse(is.null(ar2), NA, ar2),
           ifelse(is.null(disp), NA, disp),
           ifelse(is.null(loglik), NA, loglik))
  
  names(vec) <- c('AIC','Adj R2','Dispersion','Log-Lik')
  return(vec)
}
```

```{r}
# insert chunk & run with rmarkdown::render('DATA621_HW5_Group1.Rmd') to see knit vars in local environment.
#knitr::knit_exit()
```

```{r data_load}
# load data
df_train <- read.csv('https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw5/data/wine-training-data.csv')
df_predict <- read.csv('https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw5/data/wine-evaluation-data.csv')
```

## Overview

In this homework assignment we explored, analyzed and modeled a data set containing information on approximately 12,000 commercially available wines.
The variables were mostly related to the chemical properties of the wine being sold.
The response variable was the number of sample cases of wine that were purchased by wine distribution companies after a sample.

Initially, we examined the data for any problems that may have existed, such as missing data, outliers, and multi-collinearity.
Next we took the necessary steps to clean the data, and built two poisson regressions, two negative binomial regressions, and two multivariate linear regression models using the training dataset.

We trained the models and evaluated them based on how well they performed against the provided evaluation data.
Finally, we selected a final model that provided the best balance between accuracy and simplicity to predict the number of cases of wine sold given certain properties of the wine.

------------------------------------------------------------------------

## 1. Data Exploration

The **training** dataset contained `r nrow(df_train)` observations of 17 predictor variables, where each record represented a commercially available wine.

These variables included measures of acidity and amounts of various chemical compounds, as well as qualitative and marketing-related data such as reviewer starts and consumer responses to label design.

The **prediction** dataset contained `r nrow(df_predict)` observations over the same predictor variables.

-   `Target`: Number of Cases Purchased
-   `AcidIndex`: Proprietary method of testing total acidity of wine by using a weighted average
-   `Alcohol`: Alcohol Content
-   `Chlorides`: Chloride content of wine
-   `CitricAcid`: Citric Acid Content
-   `Density`: Density of Wine
-   `FixedAcidity`: Fixed Acidity of Wine
-   `FreeSulfurDioxide`: Sulfur Dioxide content of wine
-   `LabelAppeal`: Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.
-   `ResidualSugar`: Residual Sugar of wine
-   `Stars`: Wine rating by a team of experts. 4 Stars = Excellent, 1 Star = Poor. A high number of stars suggests high sales
-   `Sulphates`: Sulfate content of wine
-   `TotalSulfurDioxide`: Total Sulfur Dioxide of Wine
-   `VolatileAcidity`: Volatile Acid content of wine
-   `pH`: pH of wine

### 1.1 Summary Statistics

In order to explore summary stats and distribution characteristics of our dataset, we needed to first conduct some basic transformations and cleanup:

-   The 'training' dataset contained a single response variable `target`, a numeric variable indicating the number of cases purchased.
-   The 'prediction' dataset contained no values for `target`, suggesting this data might be used for prediction rather than validation and evaluation of model performance. For clarity we'll renamed this this dataset 'prediction' instead and created a separate validation hold-out from the training data.
-   There was a numeric `index` column labeling the observations which could be excluded from the models.
-   `r nrow(df_predict)` observations (or `r #round(nrow(df_predict)/nrow(df_all),2)*100`%) of the total dataset had been set aside for prediction.
-   The combined training and prediction datasets consist of `r #nrow(df_all)` observations containing `r #ncol(df_all)-2` predictor variables.

While exploring this data, we made the following observations:

-   Data contained only numeric values.
-   4 variables were discrete (`stars`, `labelappeal`, `acidindex`, `target`).
-   8 variables out of 14 contained missing values.
-   `target` (number of cases purchased) varied between 0 and 8.

```{r clean_data, message=FALSE, warning=FALSE, error=FALSE}
# fix index name in predict for bind
df_predict <- df_predict %>% rename(INDEX=IN)
#df_train <- df_train %>% rename(INDEX=IN)

# union for exploration and prep
df_train <- df_train %>% mutate(source='train')
df_predict <- df_predict %>% mutate(source='predict')
df_all <- bind_rows(df_train, df_predict)

# fix column labels
names(df_all) <- str_to_lower(str_replace_all(names(df_all), c(" " = "_" , "," = "", "\\*" = "", "\\(" = "", "\\)" = "", "`" = "", "\\/" = "_")))

# drop un-necessary columns
df_all <- df_all %>% dplyr::select(!index)
```

```{r summary}
# summary stats for all non-dummy variables
df_all %>% 
  dplyr::select(!c(source, target)) %>%
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table(cap='Summary Statistics')
```

------------------------------------------------------------------------

### 1.2 Distribution

One of the first characteristics that stood out was the presence of negative values for many chemical compounds, and the relative normality of their distributions.
This suggested they had already been power-transformed to produce normal distributions for modeling.

Variables related to sugars, chlorides, acidity, sulfides and sulfates all seemed to fall in this category.
Considering that were are analyzing very tiny amounts of chemical compounds, we might correctly assume their natural distributions may be highly skewed.

The variables `acidindex` (proprietary method of testing total acidity of wine by using a weighted average) seemed to be slightly right-skewed.

```{r distrib}
numeric_cols <- sapply(df_all, is.numeric)

df_all[,numeric_cols] %>%
  dplyr::select(!target) %>%
  pivot_longer(everything(),names_to = c('variables'),values_to = c('values')) %>% 
  ggplot() +
  geom_histogram(aes(x=values, y = ..density..), alpha=0.5, colour='black', size=0.2) +
  geom_density(aes(x=values), color='purple') +
  facet_wrap(vars(variables), scales="free")
```

------------------------------------------------------------------------

### 1.3 Boxplots

Graphing our variables distributions with boxplots didn't identify any outliers we needed to deal with.

We did see a relationship between `stars`/`labelappeal`and the `target` variable.
As `labelappeal` increased, the `target` variable also went up - suggesting a positive relationship between label design appeal and cases purchased.

We also noticed that missing/NA values for `stars` was associated with low values for `target`.
Using the hint from from the assignment that "sometimes, the fact that a variable is missing is actually predictive of the target", we won't discard or impute over the missing values for `stars`.

```{r boxplots}
box_plots <- df_train %>% 
  dplyr::select(!c(INDEX, source))
  
box_plots %>% 
  gather(key = 'variables', value = 'values')  %>%
  ggplot(aes(variables, values)) + 
  geom_boxplot(fill = 'coral4') + 
    labs(title = 'Boxplot: Training data',
       x = 'Variables',
       y = 'Values')+
  facet_wrap(. ~variables, scales='free', ncol=6)
```

```{r}
box_plots %>% 
  dplyr::select(c(TARGET, STARS, LabelAppeal, AcidIndex)) %>%
  pivot_longer(!c(TARGET), names_to = 'variables', values_to = 'values') %>% 
  arrange(variables, values) %>%
  ggplot(mapping = aes(x = factor(values), y = TARGET)) + 
  geom_boxplot(fill = 'coral4') +   
  labs(title = 'Boxplot: Discrete variables',
       x = 'Variables',
       y = 'Values')+
  facet_wrap(~variables, scales = 'free',  ncol = 1)
```

------------------------------------------------------------------------

### 1.4 Scatter Plots

Graphing our variables distributions with scatterplots helped us understand relationships between our independent variables and the target variable.

**ADD TEXT**

```{r}
featurePlot(box_plots[,2:ncol(box_plots)], box_plots[,1], plot = "scatter")
```

------------------------------------------------------------------------

### 1.5 Correlation Matrix

In the correlation plot below, we see that variables `Star` and `LabelAppeal` are the most positively correlated to the response variable (more stars, as a result, more purchases/ better label, more purchases).
There is slight negative correlation between `AcidIndex` and the `target` variable.
In terms of multicollinearity, we don't see high correlation between variables and there is a chance that we won't need to deal with it in our models.

```{r}
rcore <- rcorr(as.matrix(box_plots %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .5, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust", number.cex=0.7, diag=FALSE)
```

------------------------------------------------------------------------

## 2. Data Preparation

### 2.1 Transformations, Outliers

We tried exponentiation of related to sugars, chlorides, acidity, sulfides and sulfates variables by the natural log and other values, but did not arrive at an obvious or consistent transformation approach - so we may not be able to interpret model results on the scale of the original values for these variables.

**COULD YOU ADD THE TRANSFORMATIONS AND MAKE THE PARAGRAPH AS CONCLUSION?**

A number of numerical characteristics exhibit negative values that seem somewhat implausible, however, we have decided to trust that these variables have undergone log transformation and that the values are valid.

### 2.2 Missing Data

Next we'll find and impute any missing data.
There are 8 predictor variables that contain NAs:

```{r}
# working copy
df_prep <- df_all

# find columns with NAs
x <- df_prep %>%
  dplyr::select(!target) %>%
  apply(2, function(col) sum(is.na(col)))

x[x>0] %>% as.data.frame() %>% 
  setNames('is_na') %>% 
  mutate(pct = round(is_na/nrow(df_prep),2)) %>%
  arrange(desc(pct)) %>%
  nice_table(cap = 'Missing Data')
```

Heeding the warning in the assignment, *"sometimes, the fact that a variable is missing is actually predictive of the target"*, we'll consider each of these variables carefully.
While there may be data "missing completely at random" (MCAR) that we wish to impute, this may not always be the case.

------------------------------------------------------------------------

#### 2.2.1 Missing Data - Stars

The predictor `Stars` suggests that out of 16,000 wine samples, about 25% have never been professionally reviewed.
If we assume the existence of a review has some impact on the sales of a wine brand (whatever the reviewer's sentiment), then imputing mean or predicted values here might distort our model.
Therefore, we simply preserved the NAs were transformed to "0" level of `Stars` variable as the model functions automatically adjust for these observations to be used in the Zero-Inflated models.
Also, `Stars`,  `Acid Index` variables were converted from a numeric to a factor to enable further analysis.

```{r}
df_prep <- df_prep %>%
  mutate(stars = as.factor(stars))

#added None instead of NA later in the code
```


------------------------------------------------------------------------

#### 2.2.2 Missing Data - Chemical Compounds

Next we consider some of the missing chemical compounds in our wines; alcohol, sugars, chlorides, sulfites and sulfates, and measures such as `ph`.

First, we can safely assume that all wines in this dataset have an actual `ph` score greater than zero (which would represent the most acidic rank, such as powerful industrial acids). We'll want to impute more reasonable values for these.

Based on some reading into the organic wines segment, there is a growing demand in the market for specialty products such as low-sulfite, low-sugar and low-alcohol wines.
However, this still represents a very small segment of the overall market, and chemically it's not likely for these compounds to be completely absent from the final product.

Additionally, the predictors `freesulfurdioxide` and `totalsulfurdioxide` are linked - the amount of 'Free' SO2 in wine is always a subset of the 'Total' S02 present.
We only identified 59 cases where both these values were NA, while over 1500 cases had missing values for only one or the other.

Based on these observations, we'll use the MICE imputation method to predict and impute the missing values for `residualsugar`, `chlorides`, `freesulphurdioxide`, `totalsulfurdioxide`, `sulphates`, `alchohol` and `ph`.

Target/source labels and non-chemical predictors `labelappeal` and `stars` were excluded as predictors for the imputation.

```{r}
# MICE setup
init = mice(df_prep, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# omit from MICE predictors
predM[, c('target','labelappeal','stars','source')] = 0

# omit from MICE imputation
meth[c('target','labelappeal','stars','source')] = ''
```

```{r, include=FALSE}
impute = mice(df_prep, method=meth, predictorMatrix=predM, m=5)
df_prep_imputed <- complete(impute)
```

------------------------------------------------------------------------

### 2.3 Data Sparseness - Label Appeal

`labelappeal` is a numeric score of consumer ratings for a wine brand's label design.
It has also been pre-transformed to produce a normal distribution for modeling; however this is a very sparse variable with nearly half the cases having a value of zero.

This may be candidate for handling with Zero-Inflated models.
We didn't change the values here, but converted `labelappeal` from a numeric to a factor.

```{r}
df_prep_imputed <- df_prep_imputed %>%
  mutate(labelappeal = as.factor(labelappeal)) %>%
  mutate(acidindex = as.factor(acidindex)) 
```


```{r}

#dataframe for models without NA
try_df <- df_prep_imputed
# Get levels and add "0"
levels <- levels(try_df$stars)
levels[length(levels) + 1] <- "0"

# refactor Stars to include "0" as a factor level
# and replace NA with "0"
try_df$stars <- factor(try_df$stars, levels = levels)
try_df$stars[is.na(try_df$stars)] <- "0"
```

------------------------------------------------------------------------

### 2.4 Examine Final Dataset

We now have reasonably imputed values, and nearly-normal distributions for our numeric predictors, taking special note of the frequency of zero values for `labelappeal` and `stars`.

```{r}
n_missing <- apply(df_prep_imputed, 2, function(col) sum(is.na(col)))
n_zero <- colSums(df_prep_imputed==0)

df <- data.frame(n_missing, n_zero)
df <- cbind(variable=rownames(df),df)
rownames(df) <- NULL

df %>% 
  dplyr::filter(variable != 'source' & variable != 'target') %>%
  arrange(variable) %>% 
  nice_table(cap = 'Final Dataset - Missing and Zeros')
```

```{r}
numeric_cols <- sapply(df_prep_imputed, is.numeric)

df_prep_imputed[,numeric_cols] %>%
  dplyr::select(!target) %>%
  pivot_longer(everything(),names_to = c('variables'),values_to = c('values')) %>% 
  ggplot() +
  geom_histogram(aes(x=values, y = ..density..), alpha=0.5, colour='black', size=0.2) +
  geom_density(aes(x=values), color='purple') +
  facet_wrap(vars(variables), scales="free")
```

------------------------------------------------------------------------

### 2.5 Split Datasets

With transformations complete, we split back into training and prediction datasets based on our `source_flag`, and create a 15% validation hold-out from the training data.

```{r}
# random seed
set.seed(42)
# split back to train and test, remove source flag
df_train_all <- df_prep_imputed %>% filter(source == 'train') %>% dplyr::select(!source)

df_predict <- df_prep_imputed %>% filter(source == 'predict') %>% dplyr::select(!source)

# create a validation holdout from the training dataset
row_sample <- sample(c(TRUE,FALSE), nrow(df_train_all), replace=TRUE, prob=c(0.85,0.15))

df_train <- df_train_all[row_sample,]
df_valid <- df_train_all[!row_sample,]
```


```{r}
# random seed
set.seed(42)
# split back to train and test, remove source flag
try_df_all <- try_df %>% filter(source == 'train') %>% dplyr::select(!source)

try_df_predict <- try_df %>% filter(source == 'predict') %>% dplyr::select(!source)

# create a validation holdout from the training dataset
row_samples <- sample(c(TRUE,FALSE), nrow(try_df_all), replace=TRUE, prob=c(0.85,0.15))

try_train <- try_df_all[row_samples,]
try_valid <- try_df_all[!row_samples,]
```

------------------------------------------------------------------------

## 3. Build Models

### 3.1 Poisson Regression 1

Poisson Regression assumes that the variance and mean of our dependent variable `target` are roughly equal, otherwise we may be looking at over- or under-dispersion.
The data used for the model has no missing values (stars=NA was substituted with stars=None), `Label Appeal` and `Acid Index` variables were transformed to factors. 

```{r, echo=TRUE}
pr1 <- glm(target ~ ., family = 'poisson', data = try_train)
```

```{r}
summary(pr1)

pr1_diag <- model_diag(pr1)

pr1_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

Stepwise selection can help to reduce the AIC metric.
```{r pr_aic, warning=FALSE, message=FALSE}
pr1_aic <- pr1 %>% stepAIC(trace = FALSE)
summ(pr1_aic)
```

```{r}
pr1_diag <- model_diag(pr1_aic)

pr1_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```


#### Model Performance

We note that our model has generated 'dummies' from our categorical variables `labelappeal` and `stars`, and of the 14 total predictors, all but 4 have statistical significance.


The following variables had highly **positive coefficients** in our model, indicating a higher number of cases purchased. As discussed before, high rating and good labels lead to better sales:

```{r}
as.data.frame(summ(pr1_aic)$coeftable) %>%
  dplyr::select(Est.,p) %>%
  filter(p < 0.06 & Est. > 0.2) %>%
  arrange(desc(Est.)) %>%
  nice_table(cap='Positive Coefficients', cols=c('Est','p'))
```

Variables with highly **negative coefficients** indicated a lower number of cases purchased. The results followed the common logic, the higher total acidity of wine and no rating lead to low sales:

```{r}
as.data.frame(summ(pr1_aic)$coeftable) %>%
  dplyr::select(Est.,p) %>%
  filter(p < 0.06 & Est. < -0.2) %>%
  arrange(Est.) %>%
  nice_table(cap='Negative Coefficients', cols=c('Est','p'))
```

The `null deviance` of `r round(pr1$null.deviance,2)` defined how well the target variable could be predicted by a model with only an intercept term.

The `residual deviance` of `r round(pr1$deviance,2)` defined how well the target variable can be predicted by the AIC model that we fit with the predictor variables listed above.
The lower the value, the better the model's predictions of the response variable.

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r round(pr1$aic,2)`.
The lower the AIC value, the better the model's ability to fit the data.
We used this metric to compare the relative performance of the six models.

Notably, our Dispersion Parameter is `r round(pr1_diag['Dispersion'],2)`, which suggests a degree of under-dispersion in the data.


By graphing our target values (green) against our predicted values (blue) we can easily see this model tends to over-predict the higher count levels, and wildly under-predict the lower count levels.

```{r, fig.align='default', out.width='50%', fig.height=4}
# predict based on our validation holdout
pr1_valid <- predict(pr1_aic, newdata = try_valid, type = "response")

# bind the predicted and actuals for comparison
pr1_eval <- bind_cols(target = try_valid$target, predicted= pr1_valid)
#pr1_eval[is.na(pr1_eval)] <- 0


# xy plot of validation predictions and targets
pr1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
pr1_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```


#### Model Assumptions


We evaluated the modeling assumptions using standard diagnostic plots, marginal model plots, and a Variance Inflation Factor (VIF) to assess collinearity.

The resulting plot 1 below (predicted values vs. residuals) didn't show a random pattern. Examining the Standardized Pearson Residuals plot, the model seemed good until the middle range but there were extremes on the right.

```{r}
par(mfrow=c(2,2))
plot(pr1_aic)
```


All variables had a VIF less than 5. As a result, th multi-collinearity may not be a problem for this model.

```{r}
as.data.frame(car::vif(pr1_aic)) %>% 
  arrange(desc(GVIF)) %>%
  nice_table(cap='Variance Inflation Factor', cols=c('GVIF','df','adj'))
```


------------------------------------------------------------------------

### 3.2 Poisson Regression 2

We built a Zero-Inflated Poisson model to handle the large number of zero values in our `labelappeal` and `stars` predictors, to see if we can improve model accuracy.

```{r, echo=TRUE}
pr2 <- zeroinfl(target ~ . | ., data=try_train, dist = 'poisson')
```

```{r}
summary(pr2)

pr2_diag <- model_diag(pr2)

pr2_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```


#### Model Performance

We note that our model has generated 'dummies' from our categorical variables `labelappeal` and `stars`, and of the 14 total predictors, all but 4 have statistical significance.


The results for the positive/negative coefficients seem strange, high acid levels can increase the purchases, and high rating can decrease.


The `residual deviance` is `r #round(pr2$deviance,2)`

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r #round(pr2$aic,2)`.

Using a Zero-Inflated model, the Dispersion Parameter drops significantly, but we are getting a better overall result for counts of 3 or more.
By graphing our target values (green) against our predicted values (blue) we can see we are getting much greater accuracy rate for most of the mid- and upper counts.

Notably, we are still over-predicting counts of 1-2, and greatly under-predicting counts of zero. 

```{r, fig.align='default', out.width='50%', fig.height=4}
# predict based on our validation holdout
pr2_valid <- predict(pr2, newdata = try_valid, type="response")

# bind the predicted and actuals for comparison
pr2_eval <- bind_cols(target = try_valid$target, predicted= pr2_valid)
#pr2_eval[is.na(pr2_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(pr2)), resid=unname(resid(pr2))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
pr2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
pr2_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```


------------------------------------------------------------------------

### 3.3 Negative Binomial Regression 1

Generally, we would use Negative Binomial Regression in cases of over-dispersion (where the variance of our dependent variable is significantly greater than the mean). This does not appear to be the case with our dataset, but we'll apply it here and examine the results:

```{r, echo=TRUE}
nb1 <- glm.nb(target ~ ., data = try_train)
```

```{r}
summary(nb1)

nb1_diag <- model_diag(nb1)

nb1_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```


#### Model Performance

Of the total predictors, all but 4 have statistical significance.

The results for the positive/negative coefficients seemed reasonable, high acid levels and low rating can decrease the purchases, good rating and good label can increase.

```{r}
as.data.frame(summ(nb1)$coeftable) %>%
  dplyr::select(Est.,p) %>%
  filter(p < 0.06 & Est. > 0.2) %>%
  arrange(desc(Est.)) %>%
  nice_table(cap='Positive Coefficients', cols=c('Est','p'))
```

```{r}
as.data.frame(summ(nb1)$coeftable) %>%
  dplyr::select(Est.,p) %>%
  filter(p < 0.06 & Est. < -0.2) %>%
  arrange(Est.) %>%
  nice_table(cap='Negative Coefficients', cols=c('Est','p'))
```


The `residual deviance` is `r round(nb1$deviance,2)`

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r round(nb1$aic,2)`.
The lower the AIC value, the better the model's ability to fit the data.


As expected, the Negative Binomial Regression does not outperform the Poisson.

```{r, fig.align='default', out.width='50%', fig.height=4}
# predict based on our validation holdout
nb1_valid <- predict(nb1, newdata = try_valid, type="response")

# bind the predicted and actuals for comparison
nb1_eval <- bind_cols(target = try_valid$target, predicted= nb1_valid)
#nb1_eval[is.na(nb1_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(nb1)), resid=unname(resid(nb1))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
nb1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
nb1_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```



#### Model Assumptions

The resulting plot 1 below (predicted values vs. residuals) didn't show a random pattern. Examining the Standardized Pearson Residuals plot, the model seemed good until the middle range but there were extremes on the right.

```{r}
par(mfrow=c(2,2))
plot(nb1)
```


All variables had a VIF less than 5. As a result, th multi-collinearity may not be a problem for this model.

```{r }
as.data.frame(car::vif(nb1)) %>% 
  arrange(desc(GVIF)) %>%
  nice_table(cap='Variance Inflation Factor', cols=c('GVIF','df','adj'))
```

------------------------------------------------------------------------

### 3.4 Negative Binomial Regression 2

We'll build a Zero-Inflated Negative Binomial model to handle the large number of zero values in our `labelappeal` and `stars` predictors, to see if we can improve model accuracy.

```{r, echo=TRUE}
nb2 <- zeroinfl(target ~ . | ., data=try_train, dist = 'negbin')
```

```{r}
summary(nb2)

nb2_diag <- model_diag(nb2)

nb2_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

#### Model Performance

The results for the positive/negative coefficients seem strange, high acid levels can increase the purchases, and high rating can decrease.


The `residual deviance` is `r #round(nb2$deviance,2)`

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r #round(nb2$aic,2)`.


The Zero-Inflated Negative Binomial model sees similar improvement as with the Zero-Inflated Poisson, but as before does not outperform the Poisson.

```{r, fig.align='default', out.width='50%', fig.height=4}
# predict based on our validation holdout
nb2_valid <- predict(nb2, newdata = try_valid, type="response")

# bind the predicted and actuals for comparison
nb2_eval <- bind_cols(target = try_valid$target, predicted= nb2_valid)
nb2_eval[is.na(nb2_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(nb2)), resid=unname(resid(nb2))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
nb2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
nb2_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

### 3.5 Multiple Linear Regression 1

For our first Multiple Linear Regression, we'll use all predictors.

```{r, echo=TRUE}
lm1 <- lm(target ~ ., data=try_train)
```

```{r}
summary(lm1)

lm1_diag <- model_diag(lm1)

lm1_diag[c('AIC','Adj R2')] %>% 
  round(2) %>% nice_table()
```


#### Model Performance

The F-statistic was `401`, and out of the 14 variables, 5 had statistically significant p-values.

The Adjusted R-Squared of `r round(summary(lm1)$adj.r.squared,2)` explained about 54% of the total variance in the response variable `target`.

The RMSE was `r round(sqrt(mean(lm1$residuals^2)),2)`.

Positive coefficients indicated an increase in the purchases.
In this model, wines with a better rating, good label, high alcohol tended to have more purchases.

Similarly, negative coefficients lead less purchases.
In this model, wines with high acid level, no rating,high concentration of sulphates/chlorides tended to have less purchases.


```{r fig.align='left', fig.height=3, fig.width=4}
# validate and calculate RMSE
lm1_valid <- predict(lm1, newdata = try_valid)
lm1_eval <- bind_cols(target = try_valid$target, predicted=lm1_valid)
lm1_rmse <- sqrt(mean((lm1_eval$target - lm1_eval$predicted)^2)) 

# plot targets vs predicted
lm1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(lm1_rmse,1)))
```



#### Model Assumptions

An examination of the residuals indicated that the Residuals vs Fitted plot didn't show a constant variability of the residuals, but the Q-Q plot indicated a much better level of normality compared to the previous models.

The variables included in our model had a VIF of 1 (or slightly above), indicating the variables were not correlated, for the most part.

```{r, fig.align='default', out.width='50%', fig.height=5}
# residual plots
plot(lm1,ask=FALSE)
```

```{r fig.height=4}

vif_values <- vif(lm1)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=GVIF, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 
```

------------------------------------------------------------------------

### 3.6 Multiple Linear Regression 2

For our second Multiple Linear Regression, we'll add stepwise feature selection.

```{r, echo=TRUE}
lm2_all <- lm(target ~ ., data=try_train)
lm2 <- stepAIC(lm2_all, trace=FALSE, direction='both')
```

```{r}
summary(lm2)

lm2_diag <- model_diag(lm2)

lm2_diag[c('AIC','Adj R2')] %>% 
  round(2) %>% nice_table()
```


#### Model Performance

The F-statistic was `458`, and out of the 14 variables, 5 had statistically significant p-values.

The Adjusted R-Squared of `r round(summary(lm2)$adj.r.squared,2)`only explained about 54% of the total variance in the response variable `target`.

The RMSE was `r round(sqrt(mean(lm2$residuals^2)),2)`.

Positive coefficients indicated an increase in the purchases.
In this model, wines with a better rating, good label, high alcohol tended to have more purchases.

Similarly, negative coefficients lead less purchases.
In this model, wines with high acid level, no rating,high concentration of sulphates/chlorides tended to have less purchases.


```{r fig.align='left', fig.height=3, fig.width=4}
# validate and calculate RMSE
lm2_valid <- predict(lm2, newdata = try_valid)
lm2_eval <- bind_cols(target = df_valid$target, predicted=lm2_valid)
lm2_rmse <- sqrt(mean((lm2_eval$target - lm2_eval$predicted)^2)) 

# plot targets vs predicted
lm2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(lm2_rmse,1)))
```

#### Model Assumptions

An examination of the residuals indicated that the Residuals vs Fitted plot didn't show a constant variability of the residuals, but the Q-Q plot indicated a much better level of normality compared to the previous models.

The variables included in our model had a VIF of 1 (or slightly above), indicating the variables were not correlated, for the most part.

```{r, fig.align='default', out.width='50%', fig.height=5}
# residual plots
plot(lm2,ask=FALSE)
```

```{r fig.height=4}

vif_values <- vif(lm2)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=GVIF, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 
```


------------------------------------------------------------------------

### 3.7 Lasso

We used the zipath() to fit zero-inflated poisson regression models with variable selection using lasso regularization.
The count and the logit models both start with all the predictor variables and we used the coefficients parameters that generated the smallest AIC value for the model.

```{r}
fit.lasso <- zipath(target~.|.,data = df_train, family = "negbin")
```

```{r eval=TRUE}
minBic <- which.min(BIC(fit.lasso))
coef(fit.lasso, minBic)
cat("theta estimate", fit.lasso$theta[minBic])
```

```{r}
minAic <- which.min(AIC(fit.lasso))
as.data.frame(coef(fit.lasso, minAic)) %>% nice_table(cap='Zero-Inflated Model Coefficients', dig=4) 
```

**Theta Estimate:** `r cat("theta estimate", fit.lasso$theta[minAic])`

The coefficients for the count model that survive the regularization process include the dummy variables for `labelappeal`, `stars`, and the variables `density`.

-   `labelappeal` - the dummy variables derived from label appeal are strong indicators of the number of cases that will be purchased\
-   `stars` - the dummy variables derived from wine ratings are strong indicators of number of cases that will be purchased
-   `density` - is a negative indicator of the number of cases purchased by distributors suggesting that lighter wines are more popular than full-bodied wines
-   the following variables drop out of the final model `residualsugar`, `totalsulfurdioxide`, `freesulfurdioxide`, and `fixedacidity`

```{r fig.height=4}
c_df <- data.frame(enframe(coef(fit.lasso, minAic)$count[-1]))  %>% 
            arrange(value) %>% mutate_if(is.numeric,round, digits = 4)
c_df %>% 
ggplot() +
  geom_col(aes(y = reorder(name,value), x=value, fill = {value > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients (Count)"))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5),legend.position = "none")
```

The coefficients for the zero inflation model that survived the regularization process include `stars`, `labelappeal-1`, `labelappeal1` ,and `volatileacidity`.

-   `stars` - the dummy variables derived from wine ratings are strong indicators of number of cases that will be purchased
-   `labelappeal-1` is negative and `labelappeal1` is positive with no other label related dummy variable being included in the model. This would suggest that label aesthetics only count at the margins between positive and negative customer sentiment.
-   `volatileacidity` - is the only other variable coefficient that is included in the final model. With a negative coefficient lower volatile acid content is preferred when making a purchasing decision.

```{r fig.height=4}
c_df <- data.frame(enframe(coef(fit.lasso, minAic)$zero[-1])) %>% 
            arrange(value) %>% mutate_if(is.numeric,round, digits = 4)
c_df %>%
ggplot() +
  geom_col(aes(y = reorder(name,value), x=value, fill = {value > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients (Zero-Inflation)"))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5),legend.position = "none")
```

#### Diagnostics

```{r, fig.align='default', out.width='50%', fig.height=4}
df_valid_cleaned <- df_valid %>% filter(acidindex != 4)
lasso_eval <- predict(fit.lasso, newdata=df_valid_cleaned, which=minAic)
lasso_eval <- data.frame(lasso_eval)
colnames(lasso_eval) <- c('predicted')
lasso_eval$target <- df_valid_cleaned$target
lasso_eval$resid <- lasso_eval$predicted - lasso_eval$target
lasso_rmse <- sqrt(mean((lasso_eval$target - lasso_eval$predicted)^2)) 

# Diagnostic plots

# xy plot of model fitted and residuals
lasso_eval %>%
  ggplot(aes(x=predicted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
lasso_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
lasso_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

### 4. Model Selection

...

```{r}

results_lm_tbl <- tibble(
                      Model = character(),
                      mape = numeric(), 
                      smape = numeric(), 
                      mase = numeric(), 
                      mpe = numeric(), 
                      "RMSE" = numeric(),
                      AIC = numeric(),
                      "Adjusted R2" = numeric(),
                      "F-statistic" = numeric()
                )


multi_metric <- metric_set(mape, smape, mase, mpe, yardstick::rmse)


```

```{r}
## Poisson Regression 1
pr1_df <-pr1_eval %>% multi_metric(truth=target, estimate=predicted)
b <- summary(pr1)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Poisson Regression 1",
                      mape = pr1_df[[1,3]],
                      smape = pr1_df[[2,3]],
                      mase = pr1_df[[3,3]],
                      mpe = pr1_df[[4,3]],
                      "RMSE" = pr1_df[[5,3]],
                      AIC = b$aic,
                      "Adjusted R2" = NA,
                      "F-statistic" = NA
                     ))

```

```{r}
## Poisson Regression 2
pr2_df <-pr2_eval %>% multi_metric(truth=target, estimate=predicted)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Poisson Regression 2",
                      mape = pr2_df[[1,3]],
                      smape = pr2_df[[2,3]],
                      mase = pr2_df[[3,3]],
                      mpe = pr2_df[[4,3]],
                      "RMSE" = pr2_df[[5,3]],
                      AIC = AIC(pr2),
                      "Adjusted R2" = NA,
                      "F-statistic" = NA
                     ))
```

```{r}
## Negative Binomial 1
nb1_df <-nb1_eval %>% multi_metric(truth=target, estimate=predicted)
b <- summary(nb1)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Negative Binomial 1",
                      mape = nb1_df[[1,3]],
                      smape = nb1_df[[2,3]],
                      mase = nb1_df[[3,3]],
                      mpe = nb1_df[[4,3]],
                      "RMSE" = nb1_df[[5,3]],
                      AIC = b$aic,
                      "Adjusted R2" = NA,
                      "F-statistic" = NA
                     ))
```

```{r}
## Negative Binomial 2
nb2_df <-nb2_eval %>% multi_metric(truth=target, estimate=predicted)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Negative Binomial 2",
                      mape = nb2_df[[1,3]],
                      smape = nb2_df[[2,3]],
                      mase = nb2_df[[3,3]],
                      mpe = nb2_df[[4,3]],
                      "RMSE" = nb2_df[[5,3]],
                      AIC = AIC(nb2),
                      "Adjusted R2" = NA,
                      "F-statistic" = NA
                     ))
```

```{r}
# Multiple Linear Regression 1
lm1_df <-lm1_eval %>% multi_metric(truth=target, estimate=predicted)
b <- summary(lm1)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Multiple Linear Model 1",
                      mape = lm1_df[[1,3]],
                      smape = lm1_df[[2,3]],
                      mase = lm1_df[[3,3]],
                      mpe = lm1_df[[4,3]],
                      "RMSE" = nb1_df[[5,3]],
                      AIC = AIC(lm1),
                      "Adjusted R2" = b$adj.r.squared,
                      "F-statistic" = b$fstatistic[[1]]
                     ))
```

```{r}
# Multiple Linear Regression 2
lm2_df <-lm2_eval %>% multi_metric(truth=target, estimate=predicted)
b <- summary(lm2)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Multiple Linear Model 2",
                      mape = lm2_df[[1,3]],
                      smape = lm2_df[[2,3]],
                      mase = lm2_df[[3,3]],
                      mpe = lm2_df[[4,3]],
                      "RMSE" = lm2_df[[5,3]],
                      AIC = AIC(lm2),
                      "Adjusted R2" = b$adj.r.squared,
                      "F-statistic" = b$fstatistic[[1]]
                     ))
```

```{r}
# Lasso
m_df <- lasso_eval %>% multi_metric(truth=target, estimate=predicted)

results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "Lasso",
                      mape = m_df[[1,3]],
                      smape = m_df[[2,3]],
                      mase = m_df[[3,3]],
                      mpe = m_df[[4,3]],
                      "RMSE" = m_df[[5,3]],
                      AIC = as.numeric(AIC(fit.lasso)[minAic]),
                      "Adjusted R2" = NA,
                      "F-statistic" = NA
                     ))
```

```{r}
kable(results_lm_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")
```

```{r, eval=TRUE}

tab_model(pr1, nb1,
          dv.labels = c('Poisson','Negative Binomial'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.ci=FALSE, show.p = TRUE,show.stat=TRUE, digits.p=4)

```

```{r, eval=TRUE}
tab_model(lm1, lm2,
          dv.labels = c('Multiple Linear Regression 1','Multiple Linear Regression 2'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.ci=FALSE, show.p = TRUE,show.stat=TRUE, digits.p=4)
```

```{r, eval=TRUE}

tab_model(pr2, nb2,
          dv.labels = c( 'Poisson zeroinfl', 'Negative Binomial zeroinfl'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.zeroinf = TRUE, show.p = TRUE, show.ci=FALSE, show.stat=TRUE, 
          digits.p=4)

```

------------------------------------------------------------------------

# Predictions

------------------------------------------------------------------------

# Conclusion

------------------------------------------------------------------------

# Appendix

## References

**'Total Sulfur Dioxide -- Why it Matters, Too!'**\
Iowa State University\
<https://www.extension.iastate.edu/wine/total-sulfur-dioxide-why-it-matters-too/>

## R Code
